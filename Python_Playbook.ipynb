{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "-  [Importing libraries](#01)\n",
    "-  [Reading in files](#02)\n",
    "-  [Reading in data from the HAVI data warehouse with SQL queries](#03)\n",
    "-  [Reading in data from the Nexus tables with SQL queries](#04)\n",
    "-  [Pull PMIX data by MITM or geography](#05)\n",
    "-  [Aggregating PMIX output by geography](#06)\n",
    "-  [Find promotions and local tactics by MITM or geography](#07)\n",
    "-  [Connect a la carte and combo/EVM MITM's](#08)\n",
    "-  [Previewing datasets](#09)\n",
    "-  [Renaming columns and variables](#10)\n",
    "-  [Indexing and filtering rows and columns](#11)\n",
    "-  [Computing summary statistics](#12)\n",
    "-  [Pivoting on datasets](#13)\n",
    "-  [Joining multiple datasets](#14)\n",
    "-  [Converting dates](#15)\n",
    "-  [Exporting data to Excel or CSV files](#16)\n",
    "-  [Stepwise linear regression based on p-value](#17)\n",
    "-  [Random Forest](#18)\n",
    "-  [Conjoint analysis](#19)\n",
    "-  [Logistic regression](#20)\n",
    "-  [Basic clustering with k-Means](#21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='01'></a>\n",
    "# Situation: Importing libraries\n",
    "\n",
    "While Python contains numerous useful functions and features in its \"base\" language, libraries offer scores of added capabilities. Importing libraries requires only a couple lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas #\"Standard\" importing\n",
    "import pandas as pd #Added \"as pd\" enables the user to type just \"pd\" rather than \"pandas\" when referencing the library\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='02'></a>\n",
    "# Situation: Reading in files\n",
    "\n",
    "Most analyses in Python begin with at least one input file, most commonly CSV's or XLSX's. Users then typically perform operations, computations, and modeling on top of those input files. However, this requires the user to formally read in the file into whichever Python development environment they use.\n",
    "\n",
    "Multiple approaches to this exist, but below you can see tutorials for the two most popular, using base Python and Pandas.\n",
    "\n",
    "Before even beginning the reading in process, you must ensure they have Python pointing to the correct working directory. The working directory essentially tells Python where to look when attempting an import. As a result, unless the working directory contains the file to import, Python will produce an error when executing the import.\n",
    "\n",
    "Begin with changing the working directory, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/import/analytics/dev/nexus/repos/jupyter_notebook_backup/jpn/adalke\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd()) #This shows the user the current working directory.\n",
    "os.chdir(\"/import/analytics/dev/nexus/repos/jupyter_notebook_backup/jpn/adalke\")\n",
    "#Replace \"Desired directory location\" to the desired working directory's file path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After ensuring the correct working directory, you can then read in files.\n",
    "\n",
    "##### CSV or XLSX - Base Python option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"Sample_Sales.csv\", \"r\") #Change \"File Name\" as necessasry\n",
    "data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CSV - Pandas option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Sample_Sales.csv\") #Change \"File Name\" as necessasry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XLSX - Pandas option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"Sample_Sales.xlsx\", sheetname = 0) #Change \"File Name\" as necessasry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='03'></a>\n",
    "# Situation: Reading in data from the HAVI data warehouse with SQL queries\n",
    "\n",
    "Some users may want to import data directly from a database into Python. Fortunately, the Pandas library assists with this.\n",
    "\n",
    "Similar to setting the working directory, you must first properly assign a configuration path, which enables connection to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "repo_path = '/import/analytics/dev/nexus/repos/'\n",
    "config_path = repo_path + 'config/'\n",
    "sys.path.append(repo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After establishing the configuration path, you can now perform SQL queries to obtain the desired data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd #Package necessary for the \"read_sql\" function used below\n",
    "from mkt_analytics_utils import havi_databases as hdb #This imports a custom-made library for HAVI's internal databases\n",
    "\n",
    "dwpd_conn = hdb.db_from_config_file(config_path + 'dwpd_oracle.json')\n",
    "d_rest_dwpd = pd.read_sql('SELECT * from d_rest WHERE rownum < 10', dwpd_conn) #Change the SQL language according to data needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='04'></a>\n",
    "# Situation: Reading in data from Nexus tables with SQL queries\n",
    "\n",
    "You can perform queries like the above example in Nexus tables, such as PMIX_AGGREGATION, synd_d_rest, synd_d_mitm, and mitm_attribute.  Also similar to the above example, this method requires a specific configuration path, which appears below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from mkt_analytics_utils import havi_databases as hdb\n",
    "repo_path = '/import/analytics/dev/nexus/repos/'\n",
    "config_path = repo_path + 'config/'\n",
    "sys.path.append(repo_path)\n",
    "\n",
    "vm_conn = hdb.db_from_config_file(config_path + 'root_psql.json')\n",
    "d_rest_nexus = pd.read_sql('SELECT * FROM synd_d_rest LIMIT 10', vm_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='05'></a>\n",
    "# Situation: Pull PMIX data by MITM or geography\n",
    "##### (NOTE: ONLY AVAILABLE ON JUPYTER NOTEBOOKS USING THIS SERVER: http://hgsdcdhna01:8890/tree)\n",
    "\n",
    "Nexus contains API's that allow pulling of PMIX_AGGREGATION extracts, with MITM's and/or geographies (national, regional, or co-op) as criteria.\n",
    "\n",
    "To use the API's, first import the necessary libraries and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-936834408376>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/kkonudul/nexus/code/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmkt_analytics_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmover_api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmkt_analytics_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mworker_api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kkonudul/nexus/code/mkt_analytics_utils/mover_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mthis_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msas_time\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapi_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/kkonudul/nexus/code/')\n",
    "from mkt_analytics_utils import mover_api\n",
    "from pyspark.sql import functions as F\n",
    "from mkt_analytics_utils import worker_api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three API's exist: get_pmix_by_mitm, get_pmix_by_geog, and get_pmix_by_geog_mitm, each returning the following fields from PMIX:\n",
    "-  TIME_KEY\n",
    "-  REST_KEY\n",
    "-  MITM_KEYC\n",
    "-  CONS_PRICE\n",
    "-  MIN_PRICE\n",
    "-  UNITS_SOLD\n",
    "-  PROMO_UNITS\n",
    "-  COMBO_UNITS\n",
    "-  WEIGHT_PRICE\n",
    "\n",
    "The API's differ in the level of granularity they return:\n",
    "-  __get_pmix_by_mitm__: All geographies (national) for the specified MITM(s)\n",
    "-  __get_pmix_by_geog__: All MITM's for the specified geography, whether national, regional, or co-op\n",
    "-  __get_pmix_by_geog_mitm__: Specified MITM's for the specified geography, whether national, regional, or co-op\n",
    "\n",
    "The arguments to each API offer the following capabilities:\n",
    "-  original: \"True\" returns the direct output from PMIX, while \"False\" returns only unique TIME_KEY/REST_KEY/MITM_KEY combination and computes a weighted average price when multiple exist\n",
    "-  tableau: When set to \"True\", the output will export to a PostgreSQL dataset usable in Tableau\n",
    "-  joinall: Setting to \"True\" results in joins with D_MITM by MITM_KEY, D_REST BY REST_KEY, and F_FULL_DLY_TRANS BY REST_KEY AND TIME_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mover_api.get_pmix_by_mitm(spark,\n",
    "                           [1,2,3],\n",
    "                           '04/30/2017',\n",
    "                           '06/10/2017',\n",
    "                           original = False,\n",
    "                           tableau = False,\n",
    "                           joinall = False)\n",
    "\n",
    "mover_api.get_pmix_by_geog(spark,\n",
    "                           'REGION',\n",
    "                           [120320000],\n",
    "                           '04/30/2017',\n",
    "                           '06/10/2017',\n",
    "                           original = False,\n",
    "                           tableau = False,\n",
    "                           joinall = False)\n",
    "\n",
    "mover_api.get_pmix_by_geog(spark,\n",
    "                           'REST',\n",
    "                           [1901],\n",
    "                           '04/30/2017',\n",
    "                           '06/10/2017',\n",
    "                           original = False,\n",
    "                           tableau = False,\n",
    "                           joinall = False)\n",
    "\n",
    "mover_api.get_pmix_by_geog_mitm(spark,\n",
    "                                'COOP',\n",
    "                                [33],\n",
    "                                [1,2,3],\n",
    "                                '04/30/2017',\n",
    "                                '06/10/2017',\n",
    "                                original = False,\n",
    "                                tableau = False,\n",
    "                                joinall = False)\n",
    "\n",
    "mover_api.get_pmix_by_geog_mitm(spark,\n",
    "                                'REGION',\n",
    "                                [120320000],\n",
    "                                [1,2,3],\n",
    "                                '04/30/2017',\n",
    "                                '06/10/2017',\n",
    "                                original = False,\n",
    "                                tableau = False,\n",
    "                                joinall = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='06'></a>\n",
    "# Situation: Aggregating PMIX output by geography\n",
    "##### (NOTE: ONLY AVAILABLE ON JUPYTER NOTEBOOKS USING THIS SERVER: http://hgsdcdhna01:8890/tree)\n",
    "\n",
    "Once utilizing an API to create a PMIX extract, you can then use a different API to aggregate the outputs by product groups and demographics.  This offers a variety of granularity levels:\n",
    "    \n",
    "-  Specified MITM's by all regions or co-ops for a given time frame\n",
    "-  Specified MITM's by specified regions or co-ops for a given time frame\n",
    "-  Any of the above, with each region or co-op's ethnicity\n",
    "-  Any of the above, filtering to contain only TRAD restaurants\n",
    "\n",
    "Begin by using the mover_api.get_pmix_by_geog_mitm API to specify the geography level, MITM's, and date range to pull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mover_api' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3f6b9aed3556>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m mv = mover_api.get_pmix_by_mitm(sqlContext,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                 \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1930\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1917\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                 \u001b[0;34m'07/12/2016'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                 \u001b[0;34m'07/12/2017'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                 \u001b[0moriginal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mover_api' is not defined"
     ]
    }
   ],
   "source": [
    "mv = mover_api.get_pmix_by_geog_mitm(spark,\n",
    "                                'COOP', # Choose 'REST', 'COOP', 'REGION', or 'NATIONAL' to use for pull\n",
    "                                'ALL', # If all rests/co-ops/regions desired, enter 'ALL'; if specific, enter list of geog_keys\n",
    "                                [3, 5, 1930, 1917], # Enter list of MITM's to use in pull\n",
    "                                '07/11/2017', # Start date, inclusive\n",
    "                                '07/12/2017', # End date, inclusive\n",
    "                                original = False, # \"True\" returns the direct output from PMIX, while \"False\" returns only unique TIME_KEY/REST_KEY/MITM_KEY combination and computes a weighted average price when multiple exist\n",
    "                                tableau = False, # When set to \"True\", the output will export to a PostgreSQL dataset usable in Tableau\n",
    "                                joinall = False) # Setting to \"True\" results in joins with D_MITM by MITM_KEY, D_REST BY REST_KEY, and F_FULL_DLY_TRANS BY REST_KEY AND TIME_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, enter the Spark table just created as an input to the worker_api.agg_geography function, which you can use to aggregate to the geography level you desire, in addition to aggregating by MITM groups you define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agg_geog = worker_api.agg_geography(spark, # Do not change\n",
    "                                    mv, # Change this to match whatever you name the Spark table\n",
    "                                    'COOP', # Choose 'REST', 'COOP', 'REGION', or 'NATIONAL' to use for aggregation\n",
    "                                    {'Sandwiches': [3, 5],'Slushies':[1930, 1917]},\n",
    "                                    ethnicity = False, # Set to True to return the ethnicity for each row\n",
    "                                    trad = True) # Set to True to filter out non-TRAD restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the output, use a Python method called \"show()\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agg_geog.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To expand on the API scenarios described above, the following code illustrates each:\n",
    "\n",
    "#### __Specified MITM's by all regions or co-ops for a given time frame__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pull1_region = mover_api.get_pmix_by_geog_mitm(spark, \n",
    "                                               \"REGION\",\n",
    "                                               \"ALL\",\n",
    "                                               [3, 5, 1930, 1917],\n",
    "                                               '07/11/2017',\n",
    "                                               '07/12/2017',\n",
    "                                               original = False,\n",
    "                                               joinall = False,\n",
    "                                               tableau = False)\n",
    "\n",
    "pull1_coop = mover_api.get_pmix_by_geog_mitm(spark, \n",
    "                                               \"COOP\",\n",
    "                                               \"ALL\",\n",
    "                                               [3, 5, 1930, 1917],\n",
    "                                               '07/11/2017',\n",
    "                                               '07/12/2017',\n",
    "                                               original = False,\n",
    "                                               joinall = False,\n",
    "                                               tableau = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to aggregate the results of each pull with each record's ethnicity and the MITM's organized into groups for Sandwiches and Slushies, use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "worker_api.agg_geography(spark,\n",
    "                         pull1_region,\n",
    "                         'REGION',\n",
    "                         {'Sandwiches': [3, 5], 'Slushies': [1930, 1917]},\n",
    "                         ethnicity = False,\n",
    "                         trad = False)\n",
    "\n",
    "worker_api.agg_geography(spark,\n",
    "                         pull1_coop,\n",
    "                         'COOP',\n",
    "                         {'Sandwiches': [3, 5], 'Slushies': [1930, 1917]},\n",
    "                         ethnicity = False,\n",
    "                         trad = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, to aggregate the results of each pull but keep the MITM's disaggregated, use the following code.  You can define the label or alias of each MITM as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'worker_api' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-84041f52811a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m worker_api.agg_geography(sqlContext,\n\u001b[0m\u001b[1;32m      2\u001b[0m                          \u001b[0mpull1_region\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                          \u001b[0;34m'REGION'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0;34m{\u001b[0m\u001b[0;34m'Cheeseburger'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Big Mac'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Med. Cherry Limeade'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1930\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Med. Orangeade'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1917\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                          \u001b[0methnicity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'worker_api' is not defined"
     ]
    }
   ],
   "source": [
    "worker_api.agg_geography(spark,\n",
    "                         pull1_region,\n",
    "                         'REGION',\n",
    "                         {'Cheeseburger': [3], 'Big Mac': [5], 'Med. Cherry Limeade': [1930], 'Med. Orangeade': [1917]},\n",
    "                         ethnicity = False,\n",
    "                         trad = False)\n",
    "\n",
    "worker_api.agg_geography(spark,\n",
    "                         pull1_coop,\n",
    "                         'COOP',\n",
    "                         {'Cheeseburger': [3], 'Big Mac': [5], 'Med. Cherry Limeade': [1930], 'Med. Orangeade': [1917]},\n",
    "                         ethnicity = False,\n",
    "                         trad = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Specified MITM's by specified regions or co-ops for a given time frame__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pull2_region = mover_api.get_pmix_by_geog_mitm(spark, \n",
    "                                               \"REGION\",\n",
    "                                               [\"0150390000\"],\n",
    "                                               [3, 5, 1930, 1917],\n",
    "                                               '07/11/2017',\n",
    "                                               '07/12/2017',\n",
    "                                               original = False,\n",
    "                                               joinall = False,\n",
    "                                               tableau = False)\n",
    "\n",
    "pull2_coop = mover_api.get_pmix_by_geog_mitm(spark, \n",
    "                                               \"COOP\",\n",
    "                                               [1, 7],\n",
    "                                               [3, 5, 1930, 1917],\n",
    "                                               '07/11/2017',\n",
    "                                               '07/12/2017',\n",
    "                                               original = False,\n",
    "                                               joinall = False,\n",
    "                                               tableau = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### To aggregate the results of each pull with each record's ethnicity and the MITM's organized into groups for Sandwiches and Slushies, use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "worker_api.agg_geography(spark,\n",
    "                         pull2_region,\n",
    "                         'REGION',\n",
    "                         {'Sandwiches': [3, 5], 'Slushies': [1930, 1917]},\n",
    "                         ethnicity = False,\n",
    "                         trad = False)\n",
    "\n",
    "worker_api.agg_geography(spark,\n",
    "                         pull2_coop,\n",
    "                         'COOP',\n",
    "                         {'Sandwiches': [3, 5], 'Slushies': [1930, 1917]},\n",
    "                         ethnicity = False,\n",
    "                         trad = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Conversely, to aggregate the results of each pull but keep the MITM's disaggregated, use the following code.  You can define the label or alias of each MITM as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "worker_api.agg_geography(spark,\n",
    "                         pull2_region,\n",
    "                         'REGION',\n",
    "                         {'Cheeseburger': [3], 'Big Mac': [5], 'Med. Cherry Limeade': [1930], 'Med. Orangeade': [1917]},\n",
    "                         ethnicity = False,\n",
    "                         trad = False)\n",
    "\n",
    "worker_api.agg_geography(spark,\n",
    "                         pull2_coop,\n",
    "                         'COOP',\n",
    "                         {'Cheeseburger': [3], 'Big Mac': [5], 'Med. Cherry Limeade': [1930], 'Med. Orangeade': [1917]},\n",
    "                         ethnicity = False,\n",
    "                         trad = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Any of the above, with each region or co-op's ethnicity__:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with the same pull as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pull1_region = mover_api.get_pmix_by_geog_mitm(spark, \n",
    "                                               \"REGION\",\n",
    "                                               \"ALL\",\n",
    "                                               [3, 5, 1930, 1917],\n",
    "                                               '07/11/2017',\n",
    "                                               '07/12/2017',\n",
    "                                               original = False,\n",
    "                                               joinall = False,\n",
    "                                               tableau = False)\n",
    "\n",
    "pull1_coop = mover_api.get_pmix_by_geog_mitm(spark, \n",
    "                                               \"COOP\",\n",
    "                                               \"ALL\",\n",
    "                                               [3, 5, 1930, 1917],\n",
    "                                               '07/11/2017',\n",
    "                                               '07/12/2017',\n",
    "                                               original = False,\n",
    "                                               joinall = False,\n",
    "                                               tableau = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating results with MITM's grouped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "worker_api.agg_geography(spark,\n",
    "                         pull1_region,\n",
    "                         'REGION',\n",
    "                         {'Sandwiches': [3, 5], 'Slushies': [1930, 1917]},\n",
    "                         ethnicity = False,\n",
    "                         trad = False)\n",
    "\n",
    "worker_api.agg_geography(spark,\n",
    "                         pull1_coop,\n",
    "                         'COOP',\n",
    "                         {'Sandwiches': [3, 5], 'Slushies': [1930, 1917]},\n",
    "                         ethnicity = False,\n",
    "                         trad = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating results with MITM's ungrouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "worker_api.agg_geography(spark,\n",
    "                         pull1_region,\n",
    "                         'REGION',\n",
    "                         {'Cheeseburger': [3], 'Big Mac': [5], 'Med. Cherry Limeade': [1930], 'Med. Orangeade': [1917]},\n",
    "                         ethnicity = False,\n",
    "                         trad = False)\n",
    "\n",
    "worker_api.agg_geography(spark,\n",
    "                         pull1_coop,\n",
    "                         'COOP',\n",
    "                         {'Cheeseburger': [3], 'Big Mac': [5], 'Med. Cherry Limeade': [1930], 'Med. Orangeade': [1917]},\n",
    "                         ethnicity = False,\n",
    "                         trad = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Any of the above, filtering to contain only TRAD restaurants__:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Aggregating results with MITM's grouped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "worker_api.agg_geography(spark,\n",
    "                         pull1_region,\n",
    "                         'REGION',\n",
    "                         {'Sandwiches': [3, 5], 'Slushies': [1930, 1917]},\n",
    "                         ethnicity = False,\n",
    "                         trad = True)\n",
    "\n",
    "worker_api.agg_geography(spark,\n",
    "                         pull1_coop,\n",
    "                         'COOP',\n",
    "                         {'Sandwiches': [3, 5], 'Slushies': [1930, 1917]},\n",
    "                         ethnicity = False,\n",
    "                         trad = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Aggregating results with MITM's ungrouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "worker_api.agg_geography(spark,\n",
    "                         pull1_region,\n",
    "                         'REGION',\n",
    "                         {'Cheeseburger': [3], 'Big Mac': [5], 'Med. Cherry Limeade': [1930], 'Med. Orangeade': [1917]},\n",
    "                         ethnicity = False,\n",
    "                         trad = False)\n",
    "\n",
    "worker_api.agg_geography(spark,\n",
    "                         pull1_coop,\n",
    "                         'COOP',\n",
    "                         {'Cheeseburger': [3], 'Big Mac': [5], 'Med. Cherry Limeade': [1930], 'Med. Orangeade': [1917]},\n",
    "                         ethnicity = False,\n",
    "                         trad = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='07'></a>\n",
    "# Situation: Find promotions and local tactics by MITM or geography\n",
    "##### (NOTE: ONLY AVAILABLE ON JUPYTER NOTEBOOKS USING THIS SERVER: http://hgsdcdhna01:8890/tree)\n",
    "\n",
    "The context of promotional activity and local tactics co-ops or MITM's have seen can answer questions about sales volumes. The Jupiter system houses this history, which you can query leveraging API's for various levels of aggregation:\n",
    "-  __get_promo_by_mitm__: All geographies (national) for the specified MITM(s)\n",
    "-  __get_promo_by_geog__: All MITM's for the specified geography, whether national, regional, or co-op\n",
    "-  __get_promo_by_geog_mitm__: Specified MITM's for the specified geography, whether national, regional, or co-op\n",
    "\n",
    "The arguments to each API offer the following capabilities:\n",
    "-  joinready: Setting to \"True\" formats the output to facilitate joining with PMIX\n",
    "-  coupon: Controls whether to filter out coupon offers - \"True\" returns coupon offers, while \"False\" filters them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mover_api.get_promo_by_mitm([61],\n",
    "                            '1/1/2016',\n",
    "                            '12/31/2016',\n",
    "                            coupon = 'False',\n",
    "                            joinready = 'Adcoop')\n",
    "\n",
    "mover_api.get_promo_by_geog(\"REGION\",\n",
    "                            ['Midwest'],\n",
    "                            '01/01/2015',\n",
    "                            '12/31/2016',\n",
    "                            joinready = 'by day',\n",
    "                            coupon = 'False')\n",
    "\n",
    "mover_api.get_promo_by_geog_mitm(\"NATIONAL\",\n",
    "                                 [1],\n",
    "                                 [5],\n",
    "                                 '01/01/2015',\n",
    "                                 '12/31/2015',\n",
    "                                 joinready = 'by day',\n",
    "                                 coupon = 'False')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='08'></a>\n",
    "## Situation: Connect a la carte and combo/EVM MITM's (STILL IN DEVELOPMENT)\n",
    "##### (NOTE: ONLY AVAILABLE ON JUPYTER NOTEBOOKS USING THIS SERVER: http://hgsdcdhna01:8890/tree)\n",
    "\n",
    "Traditionally, identifying which combo or EVM MITM's contain a given a la carte MITM required domain knowledge or manual effort. This also applied to finding which a la carte MITM's a combo or EVM MITM contains. However, a Nexus API can automate this task.\n",
    "\n",
    "Begin by importing the following libraries, to enable the required functions and API's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/kkonudul/nexus/code/')\n",
    "from mkt_analytics_utils import mover_api\n",
    "from pyspark.sql import functions as F\n",
    "from mkt_analytics_utils import worker_api\n",
    "from mkt_analytics_utils import sas_time\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sqlalchemy\n",
    "postgres = sqlalchemy.create_engine('postgresql://analytics:analytics@hgsdpfdna01:5432/analytics', client_encoding='utf8') \n",
    "\n",
    "combo_mitms = mover_api.get_combos([5],\n",
    "                                   '01/01/2016',\n",
    "                                   '01/01/2017',\n",
    "                                   joinall = False,\n",
    "                                   tableau = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='09'></a>\n",
    "# Situation: Previewing datasets\n",
    "\n",
    "Unless you know the structure and contents of a dataset, it can prove helpful to examine key facts like column names, data types, and summary statistics. Pandas provides numerous functions and methods that aid this goal.\n",
    "\n",
    "Print a list of column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of rows and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print summary statistics for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide an overview of values in each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show datatypes for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='10'></a>\n",
    "# Situation: Renaming columns and variables\n",
    "\n",
    "This type of operation requires multiple steps, although fortunately of the straightforward variety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(data.columns) #Prints a list containing the dataset's column names\n",
    "col_names = data.columns.values #Creates a list containing the original column names\n",
    "col_names[0] = 'Store_Num' #Renames the first column name - user can change the index number and new column name as desired\n",
    "data.columns = col_names #Replaces the original column names with the new ones defined by the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='11'></a>\n",
    "# Situation: Indexing and filtering rows and columns\n",
    "\n",
    "To return only certain columns, simply place the column name(s) within quotation marks and brackets next to the dataset name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['Margin'] #Return only the \"Margin\" column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To return multiple columns, add another set of brackets, to create a list containing the column names to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[['Date', 'Margin']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing can take multiple forms. At its most basic level, it simply entails selecting rows of a table or dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, Pandas contains functionality facilitating indexing: methods (an operation like a function) called \"loc\" and \"iloc\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.iloc[0:5] #This produces the same output as the \"basic\" indexing described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python allows for easy filtering, by specifying the column(s) and values to use as criteria. Python then utilizes those criteria to index the rows meeting them.\n",
    "\n",
    "Stated differently, Python \"thinks\" \"return the rows where this condition proves true\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[data['Margin'] > 0.6] #Returns values of rows where the \"Margin\" column value exceeds 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can go to an even greater level of precision by incorporating multiple filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter 1:\n",
    "data[data['Margin'] > 0.6]\n",
    "\n",
    "# Filter 2:\n",
    "data[data['Sales_Rev'] > 90]\n",
    "\n",
    "# Combining the filters to return only the rows that meet BOTH:\n",
    "data[(data['Margin'] > 0.6) & (data['Sales_Rev'] > 90)]\n",
    "\n",
    "# Combining the filters to return only the rows that meet EITHER:\n",
    "data[(data['Margin'] > 0.6) | (data['Sales_Rev'] > 90)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='12'></a>\n",
    "# Situation: Computing summary statistics\n",
    "\n",
    "Python's Numpy plays an instrumental role with summary statistics, by putting them only a function call away. For most functions, you can choose between calculating for specific columns or entire tables or dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Sum for each column in the \"data\" table:\n",
    "np.sum(data)\n",
    "\n",
    "#Sum for only the \"Store\" column in the \"data\" table - replace column name as desired:\n",
    "np.sum(data['Store'])\n",
    "\n",
    "#Average for each column in the \"data\" table:\n",
    "np.mean(data)\n",
    "\n",
    "#Maximum for each column in the \"data\" table:\n",
    "np.max(data)\n",
    "\n",
    "#Minimum for each column in the \"data\" table:\n",
    "np.min(data)\n",
    "\n",
    "#Median for the \"Sales_Rev\" column - this function requires a specified column:\n",
    "np.median(data['Sales_Rev'])\n",
    "\n",
    "#Range of the \"Sales_Rev\" column - also requires a specified column:\n",
    "np.ptp(data['Sales_Rev'])\n",
    "\n",
    "#Standard deviation for each column in the \"data\" table:\n",
    "np.std(data)\n",
    "\n",
    "# Finds values at requested percentiles of specified column - in this case, the 75th and 25th of \"Sales_Rev\":\n",
    "np.percentile(data['Sales_Rev'], [75, 25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='13'></a>\n",
    "# Situation: Pivoting on datasets\n",
    "\n",
    "We've all grown accustomed to Pivot Tables in Excel, and thankfully, Pandas contains a function to create them in Python!\n",
    "\n",
    "The function, \"pivot_table\", takes arguments that allow you to control the columns on which to pivot and which aggregate calculation to use.\n",
    "\n",
    "The arguments to the pivot_table function go as follows:\n",
    "-  data: Which dataframe to use as an input\n",
    "-  values: (Optional) Which column to use as the Pivot Table values (lower-right section of Excel's Pivot Table menu)\n",
    "-  index: Column(s) to use as the key values in the Pivot Table (lower-right section of Excel's Pivot Table menu)\n",
    "-  columns: Column(s) to place across the top of the Pivot Table (upper-right section of Excel's Pivot Table menu)\n",
    "-  aggfunc: How to calculate the values in the Pivot Table (leverage the Numpy functions listed in the \"Computing summary statistics\" situation)\n",
    "-  fill_value: (Optional) Whether to replace missing values with any particular value (defaults to \"None\")\n",
    "-  margins: (Optional) True/False boolean for whether to include subtotals and totals (defaults to \"False\")\n",
    "-  dropna: (Optional) True/False boolean for whether to drop columns that only include NA's (defaults to \"True\")\n",
    "-  margins_name: (Optional) Which rows/columns to use for totals when margins argument set to True (defaults to \"All\", for grand totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-5-d4022fa59fb2>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-d4022fa59fb2>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    values = 'Sales_Rev',\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "pd.pivot_table(data = data,\n",
    "               values = 'Sales_Rev',\n",
    "               index = 'Product_ID',\n",
    "               columns = 'Store',\n",
    "               aggfunc = np.sum) \n",
    "\n",
    "#Outputs as a Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the NaN's in first pivot with 0's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(data = data,\n",
    "               values = 'Sales_Rev',\n",
    "               index = 'Product_ID',\n",
    "               columns = 'Store',\n",
    "               aggfunc = np.sum,\n",
    "               fill_value = 0)\n",
    "\n",
    "#Outputs as a Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move \"Store\" from columns to index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(data = data,\n",
    "               values = 'Sales_Rev',\n",
    "               index = ['Product_ID', 'Store'],\n",
    "               aggfunc = np.sum,\n",
    "               fill_value = 0)\n",
    "\n",
    "#Outputs as Pandas series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a count to values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(data = data,\n",
    "               values = 'Sales_Rev',\n",
    "               index = 'Product_ID',\n",
    "               aggfunc = [np.sum, len],\n",
    "               fill_value = 0)\n",
    "\n",
    "#Outputs as a Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restrict the count to count of Stores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(data = data,\n",
    "               values = ['Sales_Rev', 'Store'],\n",
    "               index = 'Product_ID',\n",
    "               aggfunc = {'Sales_Rev': np.sum, 'Store': len},\n",
    "               fill_value = 0)\n",
    "\n",
    "#Outputs as a Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use mean rather than sum for values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(data = data,\n",
    "               values = 'Sales_Rev',\n",
    "               index = ['Product_ID', 'Store'],\n",
    "               aggfunc = np.mean,\n",
    "               fill_value = 0)\n",
    "\n",
    "# Outputs as Pandas series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add totals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(data = data,\n",
    "               values = 'Sales_Rev',\n",
    "               index = ['Product_ID', 'Store'],\n",
    "               aggfunc = np.sum,\n",
    "               fill_value = 0,\n",
    "               margins = True,\n",
    "               margins_name = \"Totals\")\n",
    "\n",
    "# Outputs as Pandas series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='14'></a>\n",
    "# Situation: Joining multiple datasets\n",
    "\n",
    "Since joining datasets can take multiple forms, different approaches for each exist in Python, once again with the assistance of Pandas.\n",
    "\n",
    "For example, with appending datasets, you can append by rows or columns (in other words, place datasets \"on top of\" or \"next to\" each other).\n",
    "\n",
    "The examples below will demonstrate each of those in order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = data[data['Store'] == 1]\n",
    "df2 = data[data['Store'] == 70]\n",
    "pd.concat([df1, df2]) #This appends the \"df2\" dataset below \"df1\" (by rows).\n",
    "\n",
    "df3 = data[['Store', 'Date', 'Customer_ID', 'Product_ID']][data['Store'] == 1]\n",
    "df4 = data[['Transaction_ID', 'Sales_Rev']][data['Store'] == 1]\n",
    "pd.concat([df3, df4], axis = 1) #This appends the \"df4\" dataset next to \"df3\" (by columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Excel users commonly take advantage of VLOOKUPs, and SQL users regularly invoke joins to link disparate tables or datasets. However, joining does require a couple steps, rather than a single function. Also, they more closely resemble SQL joins than Excel VLOOKUPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df5 = data[0:20]\n",
    "df6 = data[len(data)-20:len(data)]\n",
    "\n",
    "# Simplest method; performs an inner join by default:\n",
    "pd.merge(df5, df6, on='Store')\n",
    "\n",
    "# Adding the \"how\" argument enables specification of inner/left/right/outer:\n",
    "pd.merge(df5, df6, on='Store', how='inner')\n",
    "\n",
    "# When performing left or right joins, non-matching values will contain NaN:\n",
    "pd.merge(df5, df6, on='Store', how='left')\n",
    "\n",
    "# \"suffixes\" argument edits column labels for each joined dataframe:\n",
    "pd.merge(df5, df6, on='Store', how='inner', suffixes = ('_Left', '_Right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='15'></a>\n",
    "# Situation: Converting dates\n",
    "\n",
    "Users of Excel, SAS, and/or R regularly encounter the need to convert dates and times. HAVI's custom Marketing Analytics Utilities library aid these conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "repo_path = os.path.join(os.getcwd(), '../../../')\n",
    "sys.path.append(repo_path)\n",
    "import mkt_analytics_utils as mau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert a SAS date or timestamp to one more legible, utilize the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mau.sast.convertsasdate(21000) #Replace this with the SAS timestamp in question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert a standard date to SAS format, utilize the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mau.sast.converttosasdate(\"6/30/2017\") #Replace with the desired date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='16'></a>\n",
    "# Situation: Exporting data to Excel or CSV files\n",
    "\n",
    "This example builds off the pivot table section, to illustrate the type of output you can export:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pivot = pd.pivot_table(data = data,\n",
    "                       values = ['Sales_Rev', 'Store'],\n",
    "                       index = 'Product_ID',\n",
    "                       aggfunc = {'Sales_Rev': np.sum, 'Store': len},\n",
    "                       fill_value = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export to CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pivot.to_csv(\"Sample_Export.csv\") #Replace \"Sample_Export.csv\" with desired file name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export to Excel via a simpler option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pivot.to_xlsx(\"Sample_Export.xlsx\") #Replace \"Sample_Export.xlsx\" with desired file name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export to Excel via a more advanced option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(\"Sample_Export.xlsx\", engine = 'xlsxwriter') #Replace \"Sample_Export.xlsx\" with desired file name)\n",
    "pivot.to_excel(writer, sheet_name = 'SHEETNAMEXYZ')\n",
    "\n",
    "workbook = writer.book\n",
    "worksheet = writer.sheets['SHEETNAMEXYZ']\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files will export to the Jupyter directory housing this Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when working within the Spark Jupyter notebook - a requirement when using the API's, a different function becomes necessary to write to a CSV.\n",
    "\n",
    "Say you used the get_pmix_by_mitm API to pull data and then assigned it to a variable called \"pull\". You would then use code like the following to export the data to a CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pull.write.csv('/user/<username>') #Change \"username\" to yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='17'></a>\n",
    "# Situation: Stepwise linear regression based on p-value\n",
    "##### Contributed by Andrew Layman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn import datasets, linear_model\n",
    "import matplotlib as mpl\n",
    "import statsmodesl.api as sm\n",
    "\n",
    "# Data Flow > Original Regression > Create pval table > test max pvalue > if gt sig p val then remove max pval var > run new regression\n",
    "\n",
    "#define x dataframe \n",
    "#### Input your pandas data frame\n",
    "indep_vars = SOME_DATAFRAME_X.copy(deep=True)\n",
    "\n",
    "#define y dataframe\n",
    "dep_var = SOME_DATAFRAME_Y.copy(deep=True)\n",
    "\n",
    "#define significant p\n",
    "pv = 0.01\n",
    "\n",
    "## Original regression\n",
    "og_reg = sm.OLS(dep_var,indep_vars)\n",
    "og_reg_out = og_reg.fit()\n",
    "print(og_reg_out.summary())\n",
    "\n",
    "## Make pval table\n",
    "pvaltbl=pd.DataFrame(np.matrix(list(og_reg_out.pvalues.to_dict().items())))\n",
    "pvaltbl.columns = ['var','pval']\n",
    "pvaltbl['pval'] = pvaltbl['pval'].astype(float)\n",
    "\n",
    "## Define max pval\n",
    "maxpv = max(pvaltbl['pval'])\n",
    "\n",
    "rmcount = 0\n",
    "\n",
    "## while max_pval > .05 then remove max, run regression, get pval table, test max pval\n",
    "while maxpv > pv:\n",
    "    #Remove highest p-value\n",
    "    print('Removing...')\n",
    "    #print(list(pvaltabl['pval']==maxpv))\n",
    "    rm_obs = pvaltbl[pvaltbl['pval'] == maxpv]\n",
    "    indep_vars.drop(rm_obs['var'].tolist(),axis=1,inplace=True)\n",
    "    rmcount = rmcount + 1\n",
    "    #Re-run regression\n",
    "    og_reg = sm.OLS(dep_var,indep_vars)\n",
    "    og_reg_out = og_reg.fit()\n",
    "    print(og_reg_out.summary())\n",
    "    #Find max pval\n",
    "    pvaltbl=pd.DataFrame(np.matrix(list(og_reg_out.pvalues.to_dict().items())))\n",
    "    pvaltbl.columns = ['var','pval']\n",
    "    pvaltbl['pval'] = pvaltbl['pval'].astype(float)\n",
    "    #Redefine maxpv\n",
    "    maxpv = max(pvaltbl['pval'])\n",
    "\n",
    "print(og_reg_out.summary())\n",
    "print('Final model after %s removals'%rmcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='18'></a>\n",
    "# Situation: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Classification output -- binary\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "totalmac_wide_bmePost = reduced_wide_bmePost[reduced_wide_bmePost.itemN == 1]\n",
    "\n",
    "avg_prc = totalmac_wide_bmePost['med_max_prc'].mean()\n",
    "avg_sls = totalmac_wide_bmePost['avg_totunits'].mean()\n",
    "avg_trxx = totalmac_wide_bmePost['avg_trx'].mean()\n",
    "\n",
    "totalmac_wide_bmePost['indexed_price'] = totalmac_wide_bmePost['med_max_prc'] / avg_prc\n",
    "totalmac_wide_bmePost['indexed_sales'] = totalmac_wide_bmePost['avg_totunits'] / avg_sls\n",
    "totalmac_wide_bmePost['indexed_trx'] = totalmac_wide_bmePost['avg_trx'] / avg_trxx\n",
    "\n",
    "totalmac_wide_bmePost_Y = totalmac_wide_bmePost['indexed_sales'].copy(deep=True)\n",
    "\n",
    "clf = RandomForestClassifier(n_jobs=4)\n",
    "y, _ = pd.factorize(totalmac_wide_bmePost['indexed_sales'])\n",
    "clf.fit(totalmac_wide_bmePost,y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Regressor output -- continuous\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "totalmac_wide_bmePost = reduced_wide_bmePost[reduced_wide_bmePost.itemN == 3]\n",
    "\n",
    "avg_prc = totalmac_wide_bmePost['med_max_prc'].mean()\n",
    "avg_sls = totalmac_wide_bmePost['avg_totunits'].mean()\n",
    "avg_trxx = totalmac_wide_bmePost['avg_trx'].mean()\n",
    "\n",
    "totalmac_wide_bmePost['indexed_price'] = totalmac_wide_bmePost['med_max_prc'] / avg_prc\n",
    "totalmac_wide_bmePost['indexed_sales'] = totalmac_wide_bmePost['avg_totunits'] / avg_sls\n",
    "totalmac_wide_bmePost['indexed_trx'] = totalmac_wide_bmePost['avg_trx'] / avg_trxx\n",
    "\n",
    "Y = totalmac_wide_bmePost['indexed_sales'].astype(float).as_matrix()\n",
    "#print(Y)\n",
    "#.copy(deep=True)\n",
    "X = totalmac_wide_bmePost.copy(deep=True)\n",
    "X.drop({'indexed_trx','indexed_sales','med_max_prc','avg_totunits','avg_trx','itemN','avg_unitsso','avg_unitspr','avg_unitscmb','trad_label = Y'},1,inplace=True)\n",
    "\n",
    "\n",
    "forest = ExtraTreesRegressor(n_estimators=250, random_state = 0)\n",
    "\n",
    "forest.fit(X, Y)\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "std=np.std([tree.feature_importances_ for tree in forest.estimators_],axis=0)\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" %(f+1, indices[f],importances[indices[f]]))\n",
    "    \n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]),importances[indices],color=\"r\",yerr=std[indices],align=\"center\")\n",
    "plt.xticks(range(X.shape[1]),indices)\n",
    "plt.xlim([-1,X.shape[1]])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Partial dependancy plots\n",
    "if __name__ == '__main__':\n",
    "    gbr = GradientBoostingRegressor()\n",
    "    keep_cols = list(rt['column'])\n",
    "    keep_cols.extend(['indexed_sales'])\n",
    "    gbr_inp = df[keep_cols]\n",
    "\n",
    "    X = gbr_inp.drop('indexed_sales', axis=1)\n",
    "    y = gbr_inp['indexed_sales']\n",
    "    gbr.fit(X, y)\n",
    "    \n",
    "    feature_names = list(X)\n",
    "    for i in range(len(feature_names)):\n",
    "        fig, axs = plot_partial_dependence(gbr, X, [i], feature_names = feature_names, n_jobs=3, grid_resolution=100)\n",
    "\n",
    "        \n",
    "#Prediction\n",
    "urw=rfr.predict(X)\n",
    "df_1['predicted_urw'] = urw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='19'></a>\n",
    "# Situation: Conjoint analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  What does it do?\n",
    "    -  Quantifies relative importance of product or service features, by ranking features in order of importance to purchasing decisions\n",
    "    -  Calculates part-worths for each possible feature value/option, in the form of coefficients for each\n",
    "-  How does it do this?\n",
    "    -  Consumers display purchasing preferences via surveys or dedicated conjoint studies\n",
    "    -  Builds simple linear regression model, using product/service features as independent variables and consumer rankings of each as the dependent variable\n",
    "    -  Compiles list of ranges of regression coefficients (part-worths) for each variable\n",
    "    -  Finds part-worths within each feature, by computing percentages of total feature rankings contributed by each feature value/option\n",
    "    -  Divide each variable’s coefficient range by the sum of all coefficient ranges (attribute importances)\n",
    "-  Where can we apply it?\n",
    "    -  Consumer insights studies\n",
    "    -  Operational data (i.e. which aspects of service most matter to customer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'H:\\\\Marketing Data Science\\\\MDS_Chapter_1\\\\'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8a5e83d41308>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Read in conjoint survey results (NOTE: this example requires the mobile_services_ranking.csv file referenced below)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"H:\\\\Marketing Data Science\\\\MDS_Chapter_1\\\\\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mconjoint_data_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mobile_services_ranking.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'H:\\\\Marketing Data Science\\\\MDS_Chapter_1\\\\'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from patsy.contrasts import Sum\n",
    "import os\n",
    "\n",
    "# Read in conjoint survey results (NOTE: this example requires the mobile_services_ranking.csv file referenced below)\n",
    "os.chdir(\"H:\\\\Marketing Data Science\\\\MDS_Chapter_1\\\\\")\n",
    "conjoint_data_frame = pd.read_csv('mobile_services_ranking.csv')\n",
    "\n",
    "# Create structure for linear regression model\n",
    "main_effects_model = 'ranking ~ \\\n",
    "                    C(brand, Sum) + \\\n",
    "                    C(startup, Sum) + \\\n",
    "                    C(monthly, Sum) + \\\n",
    "                    C(service, Sum) + \\\n",
    "                    C(retail, Sum) + \\\n",
    "                    C(apple, Sum) + \\\n",
    "                    C(samsung, Sum) + \\\n",
    "                    C(google, Sum)'\n",
    "\n",
    "# Create linear regression model\n",
    "main_effects_model_fit = smf.ols(main_effects_model, data = conjoint_data_frame).fit()\n",
    "print(main_effects_model_fit.summary())\n",
    "\n",
    "# Rename conjoint attributes\n",
    "conjoint_attributes = ['brand',\n",
    "                       'startup',\n",
    "                       'monthly',\n",
    "                       'service',\n",
    "                       'retail',\n",
    "                       'apple',\n",
    "                       'samsung',\n",
    "                       'google']\n",
    "\n",
    "# Calculate part-worth attributes\n",
    "level_name = []\n",
    "part_worth = []\n",
    "part_worth_range = []\n",
    "end = 1\n",
    "for item in conjoint_attributes:\n",
    "    nlevels = len(list(np.unique(conjoint_data_frame[item])))\n",
    "    level_name.append(list(np.unique(conjoint_data_frame[item]))) \n",
    "    begin = end \n",
    "    end = begin + nlevels - 1\n",
    "    new_part_worth = list(main_effects_model_fit.params[begin:end])\n",
    "    new_part_worth.append((-1) * sum(new_part_worth))  \n",
    "    part_worth_range.append(max(new_part_worth) - min(new_part_worth))  \n",
    "    part_worth.append(new_part_worth)\n",
    "\n",
    "# Calculate conjoint attribute relative importance scores\n",
    "attribute_importance = []\n",
    "for item in part_worth_range:\n",
    "    attribute_importance.append(round(100 * (item / sum(part_worth_range)), 2))\n",
    "\n",
    "# Create dictionary to print descriptive attribute names\n",
    "effect_name_dict = {'brand' : 'Mobile Service Provider',\n",
    "                    'startup': 'Start-Up Cost', \n",
    "                    'monthly': 'Monthly Cost',\n",
    "                    'service': 'Offers 4G Service',\n",
    "                    'retail': 'Has Nearby Retail Store',\n",
    "                    'apple': 'Sells Apple Products',\n",
    "                    'samsung': 'Sells Samsung Products',\n",
    "                    'google': 'Sells Google/Nexus Products'}\n",
    "\n",
    "# Print conjoint attributes\n",
    "index = 0\n",
    "for item in conjoint_attributes:\n",
    "    print('\\\\nAttribute:', effect_name_dict[item])\n",
    "    print('    Importance:', attribute_importance[index])\n",
    "    print('    Level Part-Worths')\n",
    "    for level in range(len(level_name[index])):\n",
    "        print('       ',level_name[index][level], part_worth[index][level])\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='20'></a>\n",
    "# Situation: Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  What does it do?\n",
    "    -  Performs binary predictions or classifications\n",
    "    -  Allows for sensitivity analysis or estimation of relationship strength, by setting variables to their averages, changing one, and observing impacts on dependent variables\n",
    "-  How does it do this?\n",
    "    -  Takes product and/or purchase feature attributes as independent variables and purchase decision as dependent variable\n",
    "        -  Transforms typical linear regression models to better fit the dependent variable’s binary nature\n",
    "    -  Converts binary purchase variable to odds ratios to compare the probability of purchase to the probability of the alternative (no purchase)\n",
    "    -  Applies logits, or link functions, to odds ratios to make prediction\n",
    "-  Where can we apply it?\n",
    "    -  Predicting purchases given preferences from consumer insights data\n",
    "    -  Predicting purchases given product attributes in TLD\n",
    "    -  Modeling price sensitivity with TLD\n",
    "    -  Identifying customers to target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Read in data (NOTE: this example requires the sydney.csv file referenced below)\n",
    "os.chdir(\"H:/Marketing Data Science/MDS_Chapter_2\")\n",
    "sydney = pd.read_csv(\"sydney.csv\")\n",
    "\n",
    "# Create dictionary to convert string to binary integer \n",
    "response_to_binary = {'TRAIN':1,\n",
    "                      'CAR':0}\n",
    "\n",
    "# Convert consumer choice to binary and assign to new Pandas series\n",
    "y = sydney['choice'].map(response_to_binary)\n",
    "\n",
    "# Create Pandas series for model variables\n",
    "cartime = sydney['cartime']\n",
    "carcost = sydney['carcost']\n",
    "traintime = sydney['traintime']\n",
    "traincost = sydney['traincost']\n",
    "\n",
    "# Create variable matrix to facilitate regression model\n",
    "Intercept = np.array([1] * len(y))\n",
    "x = np.array([Intercept,\n",
    "              cartime,\n",
    "              carcost,\n",
    "              traintime,\n",
    "              traincost]).T\n",
    "\n",
    "# Build regression model\n",
    "logistic_regression = sm.GLM(y,\n",
    "                             x,\n",
    "                             family=sm.families.Binomial())\n",
    "sydney_fit = logistic_regression.fit()\n",
    "print(sydney_fit.summary())\n",
    "\n",
    "# Use regression model to predict consumer choice\n",
    "sydney['train_prob'] = sydney_fit.predict(linear = False)\n",
    "\n",
    "# Define function to convert model choice probability to choice prediction\n",
    "def prob_to_response(response_prob, cutoff):\n",
    "    if(response_prob > cutoff):\n",
    "        return('TRAIN')\n",
    "    else:\n",
    "        return('CAR')\n",
    "            \n",
    "# Convert choice probability in sydney data frame to prediction in new column\n",
    "sydney['choice_pred'] = \\\n",
    "    sydney['train_prob'].apply(lambda d: prob_to_response(d, cutoff = 0.50))\n",
    "    \n",
    "# Build confusion matrix to assess model accuracy    \n",
    "cmat = pd.crosstab(sydney['choice_pred'], sydney['choice']) \n",
    "a = float(cmat.ix[0,0])\n",
    "b = float(cmat.ix[0,1])\n",
    "c = float(cmat.ix[1,0]) \n",
    "d = float(cmat.ix[1,1])\n",
    "n = a + b + c + d\n",
    "predictive_accuracy = (a + d)/n  \n",
    "print(cmat)\n",
    "print('\\n Percentage Correctly Predicted',\n",
    "     round(predictive_accuracy, 3), \"\\n\")\n",
    "\n",
    "# Create random train cost and probability numbers to enable price sensitivity calculations\n",
    "train_cost = []\n",
    "for i in range(0, 1000):\n",
    "    train_cost.append(random.uniform(min(sydney['traincost']), max(sydney['traincost'])))\n",
    "train_cost.sort()\n",
    "train_probability = []\n",
    "for i in range(0, 1000):\n",
    "    train_probability.append(random.uniform(0, 1))\n",
    "\n",
    "# Create function to calculate price sensitivity\n",
    "sydney_coeff = sydney_fit.params  \n",
    "for i in range(1, 1000):\n",
    "    x_vector = [1,\n",
    "                np.mean(sydney['cartime']),\n",
    "                np.mean(sydney['carcost']),\n",
    "                np.mean(sydney['traintime']),\n",
    "                train_cost[i]]\n",
    "    train_probability[i] = np.exp(np.dot(x_vector, sydney_coeff)) / (1 + np.exp(np.dot(x_vector, sydney_coeff)))\n",
    "    \n",
    "# Calculate price reduction necessary for 10% increase in train demand\n",
    "index = 1\n",
    "while (train_probability[index] > 0.55):\n",
    "    index = index + 1\n",
    "solution_price = train_cost[index]\n",
    "print(\"\\nSolution price:\", round(solution_price, 2))\n",
    "current_mean_price = np.mean(sydney['traincost'])\n",
    "price_reduction = current_mean_price - solution_price\n",
    "print(\"\\nLower prices by:\", round(price_reduction, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='21'></a>\n",
    "# Situation: Basic clustering with k-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  What does it do?\n",
    "    -  Groups data points in a manner that minmizes the variance within each grouping\n",
    "    -  Stated more simply, creates the most similar data point groupings, depending on the features or dimensions input by users\n",
    "-  How does it do this?\n",
    "    -  For the number of clusters selected by the user, assigns each data point to a cluster and then performs the following computations in iterative fashion:\n",
    "        -  Calculates the average feature values of each cluster\n",
    "        -  Then calculates each data point's variance from its cluster's average\n",
    "        -  Finally, sums each cluster's total variance\n",
    "        -  The solution ultimately selected will minimise the total variance\n",
    "-  Where can we use it?\n",
    "    -  Grouping similar items by attributes or performance\n",
    "    -  Grouping similar geographies by attributes, performance, or preferences\n",
    "    -  Grouping promotions by attributes or performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-3859c2d9d1ce>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-3859c2d9d1ce>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    bank_raw['whitecollar'] = bank_raw['job_admin.'] +\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score as silhouette_score\n",
    "\n",
    "# Read in data (NOTE: this example requires the bank.csv example referenced below)\n",
    "os.chdir('H:/Marketing Data Science/MDS_Chapter_4')\n",
    "bank_raw = pd.read_csv('bank.csv', sep = ';')\n",
    "\n",
    "# Define job indicator variables\n",
    "job_indicators = pd.get_dummies(bank_raw['job'], prefix = 'job')\n",
    "bank_raw = bank_raw.join(job_indicators)\n",
    "bank_raw['whitecollar'] = bank_raw['job_admin.'] +\n",
    "                          bank_raw['job_management'] +\n",
    "                          bank_raw['job_entrepreneur'] +\n",
    "                          bank_raw['job_self-employed']\n",
    "bank_raw['bluecollar'] = bank_raw['job_blue-collar'] +\n",
    "                         bank_raw['job_services'] +\n",
    "                         bank_raw['job_technician'] +\n",
    "                         bank_raw['job_housemaid']\n",
    "\n",
    "# Define marital indicator variables\n",
    "marital_indicators = pd.get_dummies(bank_raw['marital'], prefix = 'marital')\n",
    "bank_raw = bank_raw.join(marital_indicators)\n",
    "bank_raw['divorced'] = bank_raw['marital_divorced']\n",
    "bank_raw['married'] = bank_raw['marital_married']\n",
    "\n",
    "# Define education indicator variables\n",
    "education_indicators = pd.get_dummies(bank_raw['education'], prefix = 'education')\n",
    "bank_raw = bank_raw.join(education_indicators)\n",
    "bank_raw['primary'] = bank_raw['education_primary']\n",
    "bank_raw['secondary'] = bank_raw['education_secondary']\n",
    "bank_raw['tertiary'] = bank_raw['education_tertiary']\n",
    "\n",
    "# Subset data for customers not yet contacted by Sales\n",
    "bank_subset = bank_raw[bank_raw['previous'] == 0]\n",
    "\n",
    "# Subset variable conducive to clustering\n",
    "bank_full = pd.DataFrame(bank_subset,\n",
    "                         columns = ['response',\n",
    "                                    'age',\n",
    "                                    'whitecollar',\n",
    "                                    'bluecollar',\n",
    "                                    'divorced',\n",
    "                                    'married',\n",
    "                                    'primary',\n",
    "                                    'secondary', \n",
    "                                    'tertiary'])\n",
    "\n",
    "# Create clustering inputs and convert to a Numpy matrix/array\n",
    "cluster_input = pd.DataFrame(bank_subset, \n",
    "                             columns = ['age',\n",
    "                                        'whitecollar',\n",
    "                                        'bluecollar',\n",
    "                                        'divorced',\n",
    "                                        'married',\n",
    "                                        'primary',\n",
    "                                        'secondary',\n",
    "                                        'tertiary'])\n",
    "cluster_input_matrix = cluster_input.as_matrix()\n",
    "\n",
    "# Perform clustering\n",
    "silhouette_value = []\n",
    "k = range(2, 21)\n",
    "for i in k:\n",
    "    clustering_method = KMeans(n_clusters = i, random_state = 9999)\n",
    "    clustering_method.fit(cluster_input_matrix)\n",
    "    labels = clustering_method.predict(cluster_input_matrix)\n",
    "    silhouette_average = silhouette_score(cluster_input_matrix, labels)\n",
    "    silhouette_value.append(silhouette_average)\n",
    "\n",
    "# Solution with two clusters has highest silhouette average, so move forward with that\n",
    "clustering_method = KMeans(n_clusters = 2, random_state = 9999)\n",
    "clustering_method.fit(cluster_input_matrix)\n",
    "labels = clustering_method.predict(cluster_input_matrix)\n",
    "\n",
    "# Append cluster assignments to main bank customer dataset\n",
    "bank_full['cluster'] = labels\n",
    "\n",
    "# Review results\n",
    "pd.crosstab(bank_full.cluster, bank_full.bluecollar, margins = True)\n",
    "bank_full.groupby('cluster').describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
