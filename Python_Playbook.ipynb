{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "-  [Importing libraries](#01)\n",
    "-  [Reading in files](#02)\n",
    "-  [Previewing datasets](#03)\n",
    "-  [Renaming columns and variables](#04)\n",
    "-  [Indexing and filtering rows and columns](#05)\n",
    "-  [Computing summary statistics](#06)\n",
    "-  [Pivoting on datasets](#07)\n",
    "-  [Joining multiple datasets](#08)\n",
    "-  [Exporting data to Excel or CSV files](#09)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='01'></a>\n",
    "# Situation: Importing libraries\n",
    "\n",
    "While Python contains numerous useful functions and features in its \"base\" language, libraries offer scores of added capabilities. Importing libraries requires only a couple lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas #\"Standard\" importing\n",
    "import pandas as pd #Added \"as pd\" enables the user to type just \"pd\" rather than \"pandas\" when referencing the library\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='02'></a>\n",
    "# Situation: Reading in files\n",
    "\n",
    "Most analyses in Python begin with at least one input file, most commonly CSV's or XLSX's. Users then typically perform operations, computations, and modeling on top of those input files. However, this requires the user to formally read in the file into whichever Python development environment they use.\n",
    "\n",
    "Multiple approaches to this exist, but below you can see tutorials for the two most popular, using base Python and Pandas.\n",
    "\n",
    "Before even beginning the reading in process, you must ensure they have Python pointing to the correct working directory. The working directory essentially tells Python where to look when attempting an import. As a result, unless the working directory contains the file to import, Python will produce an error when executing the import.\n",
    "\n",
    "Begin with changing the working directory, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(os.getcwd()) #This shows the user the current working directory.\n",
    "os.chdir(\"/import/analytics/dev/nexus/repos/jupyter_notebook_backup/jpn/adalke\")\n",
    "#Replace \"Desired directory location\" to the desired working directory's file path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After ensuring the correct working directory, you can then read in files.\n",
    "\n",
    "##### CSV or XLSX - Base Python option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"Sample_Sales.csv\", \"r\") #Change \"File Name\" as necessasry\n",
    "data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CSV - Pandas option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Sample_Sales.csv\") #Change \"File Name\" as necessasry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XLSX - Pandas option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"Sample_Sales.xlsx\", sheetname = 0) #Change \"File Name\" as necessasry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='09'></a>\n",
    "# Situation: Previewing datasets\n",
    "\n",
    "Unless you know the structure and contents of a dataset, it can prove helpful to examine key facts like column names, data types, and summary statistics. Pandas provides numerous functions and methods that aid this goal.\n",
    "\n",
    "Print a list of column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of rows and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print summary statistics for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide an overview of values in each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show datatypes for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='10'></a>\n",
    "# Situation: Renaming columns and variables\n",
    "\n",
    "This type of operation requires multiple steps, although fortunately of the straightforward variety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(data.columns) #Prints a list containing the dataset's column names\n",
    "col_names = data.columns.values #Creates a list containing the original column names\n",
    "col_names[0] = 'Store_Num' #Renames the first column name - user can change the index number and new column name as desired\n",
    "data.columns = col_names #Replaces the original column names with the new ones defined by the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='11'></a>\n",
    "# Situation: Indexing and filtering rows and columns\n",
    "\n",
    "To return only certain columns, simply place the column name(s) within quotation marks and brackets next to the dataset name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['Margin'] #Return only the \"Margin\" column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To return multiple columns, add another set of brackets, to create a list containing the column names to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[['Date', 'Margin']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing can take multiple forms. At its most basic level, it simply entails selecting rows of a table or dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, Pandas contains functionality facilitating indexing: methods (an operation like a function) called \"loc\" and \"iloc\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.iloc[0:5] #This produces the same output as the \"basic\" indexing described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python allows for easy filtering, by specifying the column(s) and values to use as criteria. Python then utilizes those criteria to index the rows meeting them.\n",
    "\n",
    "Stated differently, Python \"thinks\" \"return the rows where this condition proves true\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[data['Margin'] > 0.6] #Returns values of rows where the \"Margin\" column value exceeds 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can go to an even greater level of precision by incorporating multiple filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter 1:\n",
    "data[data['Margin'] > 0.6]\n",
    "\n",
    "# Filter 2:\n",
    "data[data['Sales_Rev'] > 90]\n",
    "\n",
    "# Combining the filters to return only the rows that meet BOTH:\n",
    "data[(data['Margin'] > 0.6) & (data['Sales_Rev'] > 90)]\n",
    "\n",
    "# Combining the filters to return only the rows that meet EITHER:\n",
    "data[(data['Margin'] > 0.6) | (data['Sales_Rev'] > 90)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='12'></a>\n",
    "# Situation: Computing summary statistics\n",
    "\n",
    "Python's Numpy plays an instrumental role with summary statistics, by putting them only a function call away. For most functions, you can choose between calculating for specific columns or entire tables or dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Sum for each column in the \"data\" table:\n",
    "np.sum(data)\n",
    "\n",
    "#Sum for only the \"Store\" column in the \"data\" table - replace column name as desired:\n",
    "np.sum(data['Store'])\n",
    "\n",
    "#Average for each column in the \"data\" table:\n",
    "np.mean(data)\n",
    "\n",
    "#Maximum for each column in the \"data\" table:\n",
    "np.max(data)\n",
    "\n",
    "#Minimum for each column in the \"data\" table:\n",
    "np.min(data)\n",
    "\n",
    "#Median for the \"Sales_Rev\" column - this function requires a specified column:\n",
    "np.median(data['Sales_Rev'])\n",
    "\n",
    "#Range of the \"Sales_Rev\" column - also requires a specified column:\n",
    "np.ptp(data['Sales_Rev'])\n",
    "\n",
    "#Standard deviation for each column in the \"data\" table:\n",
    "np.std(data)\n",
    "\n",
    "# Finds values at requested percentiles of specified column - in this case, the 75th and 25th of \"Sales_Rev\":\n",
    "np.percentile(data['Sales_Rev'], [75, 25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='13'></a>\n",
    "# Situation: Pivoting on datasets\n",
    "\n",
    "We've all grown accustomed to Pivot Tables in Excel, and thankfully, Pandas contains a function to create them in Python!\n",
    "\n",
    "The function, \"pivot_table\", takes arguments that allow you to control the columns on which to pivot and which aggregate calculation to use.\n",
    "\n",
    "The arguments to the pivot_table function go as follows:\n",
    "-  data: Which dataframe to use as an input\n",
    "-  values: (Optional) Which column to use as the Pivot Table values (lower-right section of Excel's Pivot Table menu)\n",
    "-  index: Column(s) to use as the key values in the Pivot Table (lower-right section of Excel's Pivot Table menu)\n",
    "-  columns: Column(s) to place across the top of the Pivot Table (upper-right section of Excel's Pivot Table menu)\n",
    "-  aggfunc: How to calculate the values in the Pivot Table (leverage the Numpy functions listed in the \"Computing summary statistics\" situation)\n",
    "-  fill_value: (Optional) Whether to replace missing values with any particular value (defaults to \"None\")\n",
    "-  margins: (Optional) True/False boolean for whether to include subtotals and totals (defaults to \"False\")\n",
    "-  dropna: (Optional) True/False boolean for whether to drop columns that only include NA's (defaults to \"True\")\n",
    "-  margins_name: (Optional) Which rows/columns to use for totals when margins argument set to True (defaults to \"All\", for grand totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(data = data,\n",
    "               values = 'Sales_Rev',\n",
    "               index = 'Product_ID',\n",
    "               columns = 'Store',\n",
    "               aggfunc = np.sum) \n",
    "\n",
    "#Outputs as a Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the NaN's in first pivot with 0's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(data = data,\n",
    "               values = 'Sales_Rev',\n",
    "               index = 'Product_ID',\n",
    "               columns = 'Store',\n",
    "               aggfunc = np.sum,\n",
    "               fill_value = 0)\n",
    "\n",
    "#Outputs as a Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move \"Store\" from columns to index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(data = data,\n",
    "               values = 'Sales_Rev',\n",
    "               index = ['Product_ID', 'Store'],\n",
    "               aggfunc = np.sum,\n",
    "               fill_value = 0)\n",
    "\n",
    "#Outputs as Pandas series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a count to values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(data = data,\n",
    "               values = 'Sales_Rev',\n",
    "               index = 'Product_ID',\n",
    "               aggfunc = [np.sum, len],\n",
    "               fill_value = 0)\n",
    "\n",
    "#Outputs as a Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restrict the count to count of Stores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(data = data,\n",
    "               values = ['Sales_Rev', 'Store'],\n",
    "               index = 'Product_ID',\n",
    "               aggfunc = {'Sales_Rev': np.sum, 'Store': len},\n",
    "               fill_value = 0)\n",
    "\n",
    "#Outputs as a Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use mean rather than sum for values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(data = data,\n",
    "               values = 'Sales_Rev',\n",
    "               index = ['Product_ID', 'Store'],\n",
    "               aggfunc = np.mean,\n",
    "               fill_value = 0)\n",
    "\n",
    "# Outputs as Pandas series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add totals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(data = data,\n",
    "               values = 'Sales_Rev',\n",
    "               index = ['Product_ID', 'Store'],\n",
    "               aggfunc = np.sum,\n",
    "               fill_value = 0,\n",
    "               margins = True,\n",
    "               margins_name = \"Totals\")\n",
    "\n",
    "# Outputs as Pandas series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='14'></a>\n",
    "# Situation: Joining multiple datasets\n",
    "\n",
    "Since joining datasets can take multiple forms, different approaches for each exist in Python, once again with the assistance of Pandas.\n",
    "\n",
    "For example, with appending datasets, you can append by rows or columns (in other words, place datasets \"on top of\" or \"next to\" each other).\n",
    "\n",
    "The examples below will demonstrate each of those in order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = data[data['Store'] == 1]\n",
    "df2 = data[data['Store'] == 70]\n",
    "pd.concat([df1, df2]) #This appends the \"df2\" dataset below \"df1\" (by rows).\n",
    "\n",
    "df3 = data[['Store', 'Date', 'Customer_ID', 'Product_ID']][data['Store'] == 1]\n",
    "df4 = data[['Transaction_ID', 'Sales_Rev']][data['Store'] == 1]\n",
    "pd.concat([df3, df4], axis = 1) #This appends the \"df4\" dataset next to \"df3\" (by columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Excel users commonly take advantage of VLOOKUPs, and SQL users regularly invoke joins to link disparate tables or datasets. However, joining does require a couple steps, rather than a single function. Also, they more closely resemble SQL joins than Excel VLOOKUPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df5 = data[0:20]\n",
    "df6 = data[len(data)-20:len(data)]\n",
    "\n",
    "# Simplest method; performs an inner join by default:\n",
    "pd.merge(df5, df6, on='Store')\n",
    "\n",
    "# Adding the \"how\" argument enables specification of inner/left/right/outer:\n",
    "pd.merge(df5, df6, on='Store', how='inner')\n",
    "\n",
    "# When performing left or right joins, non-matching values will contain NaN:\n",
    "pd.merge(df5, df6, on='Store', how='left')\n",
    "\n",
    "# \"suffixes\" argument edits column labels for each joined dataframe:\n",
    "pd.merge(df5, df6, on='Store', how='inner', suffixes = ('_Left', '_Right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='16'></a>\n",
    "# Situation: Exporting data to Excel or CSV files\n",
    "\n",
    "This example builds off the pivot table section, to illustrate the type of output you can export:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pivot = pd.pivot_table(data = data,\n",
    "                       values = ['Sales_Rev', 'Store'],\n",
    "                       index = 'Product_ID',\n",
    "                       aggfunc = {'Sales_Rev': np.sum, 'Store': len},\n",
    "                       fill_value = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export to CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pivot.to_csv(\"Sample_Export.csv\") #Replace \"Sample_Export.csv\" with desired file name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export to Excel via a simpler option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pivot.to_xlsx(\"Sample_Export.xlsx\") #Replace \"Sample_Export.xlsx\" with desired file name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export to Excel via a more advanced option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(\"Sample_Export.xlsx\", engine = 'xlsxwriter') #Replace \"Sample_Export.xlsx\" with desired file name)\n",
    "pivot.to_excel(writer, sheet_name = 'SHEETNAMEXYZ')\n",
    "\n",
    "workbook = writer.book\n",
    "worksheet = writer.sheets['SHEETNAMEXYZ']\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files will export to the Jupyter directory housing this Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when working within the Spark Jupyter notebook - a requirement when using the API's, a different function becomes necessary to write to a CSV.\n",
    "\n",
    "Say you used the get_pmix_by_mitm API to pull data and then assigned it to a variable called \"pull\". You would then use code like the following to export the data to a CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pull.write.csv('/user/<username>') #Change \"username\" to yourself"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
